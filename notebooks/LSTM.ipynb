{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPRsETqu/IZ6wbL3M5QYJR/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LSTM"],"metadata":{"id":"qyKpoX-lxWxT"}},{"cell_type":"markdown","source":["## 依旧先构建Vocab！"],"metadata":{"id":"jwFud8JKxZ7T"}},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WXIDL7JexVkC","executionInfo":{"status":"ok","timestamp":1769495096772,"user_tz":-480,"elapsed":62,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}},"outputId":"b8ad9d0b-ad82-41bb-b9bc-bfaf4c464c52"},"outputs":[{"output_type":"stream","name":"stdout","text":["===== Vocabulary Preview =====\n","index -> token\n","  0 ->  \n","  1 -> <unk>\n","  2 -> <bos>\n","  3 -> <eos>\n","  4 -> the\n","  5 -> and\n","  6 -> a\n","  7 -> i\n","  8 -> of\n","  9 -> to\n","now -> 74\n","unknown -> 1\n","sentence -> [2, 1686, 29, 3]\n"]}],"source":["import collections\n","import re\n","\n","file_path = 'novel.txt'\n","\n","def read_txt_file(file_path):\n","    with open(file_path, 'r') as f:\n","        lines = f.readlines()\n","    cleaned_lines = [\n","        re.sub('[^A-Za-z]+', ' ', line).strip().lower()\n","        for line in lines\n","    ]\n","    return cleaned_lines\n","\n","lines = read_txt_file(file_path)\n","\n","def tokenize(lines, token='word'):\n","    if token == 'word':\n","        return [line.split() for line in lines]\n","    elif token == 'char':\n","        return [list(line) for line in lines]\n","    else:\n","        raise ValueError(token)\n","\n","def count_corpus(corpus):\n","    \"\"\"\n","    统计语料中每个 token 出现的次数\n","    tokens:\n","        - 可以是 ['a', 'b', 'c']\n","        - 也可以是 [['a','b'], ['c','d']]\n","    返回：\n","        Counter({'a': 3, 'b': 2, ...})\n","    \"\"\"\n","    all_tokens = []\n","    for line in corpus:          # 一行一行取\n","      for token in line.split():       # 行里一个个单词取\n","          all_tokens.append(token)\n","\n","\n","    return collections.Counter(all_tokens)\n","\n","class Vocab:\n","    def __init__(self, tokens=None):\n","        \"\"\"\n","        构建词表\n","        tokens: token 列表（可以是一维或二维）\n","        \"\"\"\n","        if tokens is None:\n","            tokens = []\n","\n","        # 1. 统计词频\n","        counter = count_corpus(tokens)\n","\n","        # 2. 初始化特殊符号\n","        self.idx_to_token = [' ', '<unk>', '<bos>', '<eos>']\n","        self.token_to_idx = {\n","            ' ': 0,\n","            '<unk>': 1,\n","            '<bos>': 2,\n","            '<eos>': 3,\n","        }\n","\n","        # 3. 按频率从高到低加入普通 token\n","        for token, freq in counter.most_common():\n","            if token not in self.token_to_idx:\n","                self.idx_to_token.append(token)\n","                self.token_to_idx[token] = len(self.idx_to_token) - 1\n","\n","    def __len__(self):\n","      return len(self.idx_to_token)\n","\n","    def __getitem__(self, tokens):\n","      # 单个 token\n","      if not isinstance(tokens, (list, tuple)):\n","          return self.token_to_idx.get(tokens, self.token_to_idx['<unk>'])\n","\n","      # token 列表\n","      indices = []\n","      for token in tokens:\n","          indices.append(self[token])\n","      return indices\n","\n","\n","    def print_vocab(self, n=10):\n","      print(\"===== Vocabulary Preview =====\")\n","      print(\"index -> token\")\n","      for i in range(min(n, len(self.idx_to_token))):\n","          print(f\"{i:>3} -> {self.idx_to_token[i]}\")\n","\n","vocab = Vocab(lines)\n","\n","vocab.print_vocab(n=10)\n","\n","print(\"now ->\", vocab['now'])\n","print(\"unknown ->\", vocab['xyz'])\n","print(\"sentence ->\", vocab[['<bos>', 'dear', 'gatsby', '<eos>']])"]},{"cell_type":"markdown","source":["## 以及依旧构建dataloader！！"],"metadata":{"id":"xDPm085EyeFQ"}},{"cell_type":"code","source":["import torch\n","def build_corpus_ids(lines, vocab):\n","    # 把text转为数字\n","    words = []\n","    for line in lines:\n","        if line.strip():\n","            words += line.split()\n","    return torch.tensor([vocab[w] for w in words], dtype=torch.long)\n","\n","\n","def train_iter_sequential_simple(corpus_ids, batch_size, num_steps, device='cpu'):\n","    corpus_ids = corpus_ids.to(device)\n","    N = corpus_ids.numel()\n","    assert N > batch_size * (num_steps + 1), \"语料太短，batch_size*num_steps 太大了\"\n","\n","    # 截断到能整除 batch_size\n","    n = (N - 1) // batch_size * batch_size\n","    Xs = corpus_ids[:n].reshape(batch_size, -1)\n","    Ys = corpus_ids[1:n+1].reshape(batch_size, -1)\n","\n","    batches = []   # ★ 所有 batch 放在这里\n","\n","    L = Xs.shape[1]\n","    for t in range(0, L - num_steps + 1, num_steps):\n","        X = Xs[:, t:t+num_steps]\n","        Y = Ys[:, t:t+num_steps]\n","        batches.append((X, Y))   # ★ 收集起来\n","\n","    return batches\n","\n","\n","\n","\n","corpus_ids = build_corpus_ids(lines, vocab)\n","print(\"Total tokens:\", corpus_ids.shape)\n","\n","batch_size = 32\n","num_steps = 35\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Using device:\", device)\n","\n","train_iter = train_iter_sequential_simple(\n","    corpus_ids, batch_size, num_steps, device\n",")\n","\n","\n","for X, Y in train_iter:\n","    print(\"X shape:\", X.shape)   # (batch_size, num_steps)\n","    print(\"Y shape:\", Y.shape)\n","    print(\"X[0]:\", X[0])\n","    print(\"Y[0]:\", Y[0])\n","    break\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ahs-Wqvwyg3P","executionInfo":{"status":"ok","timestamp":1769495096821,"user_tz":-480,"elapsed":48,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}},"outputId":"3fc55eec-c599-46a7-a95a-dece29f2af7c"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Total tokens: torch.Size([53029])\n","Using device: cpu\n","X shape: torch.Size([32, 35])\n","Y shape: torch.Size([32, 35])\n","X[0]: tensor([   4,   81,   76,  487,    8,    4,  223,   29,   38,  487,   58,   25,\n","           4,  299,    8,  419, 1028,   10,    4,  420,  326,    5,  194,   92,\n","        1513,    8,    4,  201,   19,   63, 1219,    5,   18,  273,   63])\n","Y[0]: tensor([  81,   76,  487,    8,    4,  223,   29,   38,  487,   58,   25,    4,\n","         299,    8,  419, 1028,   10,    4,  420,  326,    5,  194,   92, 1513,\n","           8,    4,  201,   19,   63, 1219,    5,   18,  273,   63, 1965])\n"]}]},{"cell_type":"markdown","source":["## 初始化LSTM参数！"],"metadata":{"id":"WthfPHc41JFv"}},{"cell_type":"code","source":["\n","import math\n","import time\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","def get_lstm_params(vocab_size, num_hiddens, device):\n","    \"\"\"\n","    初始化 LSTM 模型参数（手写版）\n","    - 使用 one-hot 编码，因此输入维度 = 词表大小\n","    \"\"\"\n","\n","    # LSTM 的输入维度等于输出维度（one-hot）\n","    num_inputs = num_outputs = vocab_size\n","\n","    def normal(shape):\n","        \"\"\"小尺度正态分布初始化权重\"\"\"\n","        return torch.randn(size=shape, device=device) * 0.01\n","\n","    def two_W_one_b():\n","        \"\"\"\n","        初始化 LSTM 的三个关键参数：\n","        1. W_x：输入到隐藏状态的权重\n","        2. W_h：隐藏状态到隐藏状态的权重\n","        3. b  ：偏置项（初始化为 0）\n","\n","        维度规则：\n","        - W_x: (num_inputs, num_hiddens)\n","        - W_h: (num_hiddens, num_hiddens)\n","        - b  : (num_hiddens,)\n","        \"\"\"\n","        return (\n","            normal((num_inputs, num_hiddens)),      # W_x\n","            normal((num_hiddens, num_hiddens)),     # W_h\n","            torch.zeros(num_hiddens, device=device) # b\n","        )\n","\n","    # -------- LSTM 四组门控参数 --------\n","    W_xi, W_hi, b_i = two_W_one_b()  # 输入门 (Input Gate)\n","    W_xf, W_hf, b_f = two_W_one_b()  # 遗忘门 (Forget Gate)\n","    W_xo, W_ho, b_o = two_W_one_b()  # 输出门 (Output Gate)\n","    W_xc, W_hc, b_c = two_W_one_b()  # 候选记忆单元 (Candidate Cell)\n","\n","    # -------- 输出层参数 --------\n","    W_hq = normal((num_hiddens, num_outputs))      # 隐藏层 → 输出层\n","    b_q = torch.zeros(num_outputs, device=device)  # 输出层偏置\n","\n","    # 将所有参数打包，便于优化\n","    params = [\n","        W_xi, W_hi, b_i,   # 输入门\n","        W_xf, W_hf, b_f,   # 遗忘门\n","        W_xo, W_ho, b_o,   # 输出门\n","        W_xc, W_hc, b_c,   # 候选记忆\n","        W_hq, b_q          # 输出层\n","    ]\n","\n","    # 开启梯度\n","    for param in params:\n","        param.requires_grad_(True)\n","\n","    return params\n"],"metadata":{"id":"jdl8jlYm1K3u","executionInfo":{"status":"ok","timestamp":1769495096843,"user_tz":-480,"elapsed":18,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## 定义LSTM前传函数！"],"metadata":{"id":"i2VjpTnv6noG"}},{"cell_type":"code","source":["import torch\n","\n","# ===============================\n","# 初始化 LSTM 的隐状态\n","# ===============================\n","# 长期记忆 C 和短期记忆 H 都需要初始化\n","def init_lstm_state(batch_size, num_hiddens, device):\n","    return (\n","        torch.zeros((batch_size, num_hiddens), device=device),\n","        torch.zeros((batch_size, num_hiddens), device=device)\n","    )\n","\n","\n","# ===============================\n","# LSTM 前向传播\n","# ===============================\n","def lstm(inputs, state, params):\n","    [\n","        W_xi, W_hi, b_i,\n","        W_xf, W_hf, b_f,\n","        W_xo, W_ho, b_o,\n","        W_xc, W_hc, b_c,\n","        W_hq, b_q\n","    ] = params\n","\n","    H, C = state\n","    outputs = []\n","\n","    for X in inputs:\n","        # 1. 输入门\n","        # I_t = sigmoid(W_xi * X_t + W_hi * H_{t-1} + b_i)\n","        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)\n","\n","        # 2. 遗忘门\n","        # F_t = sigmoid(W_xf * X_t + W_hf * H_{t-1} + b_f)\n","        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)\n","\n","        # 3. 输出门\n","        # O_t = sigmoid(W_xo * X_t + W_ho * H_{t-1} + b_o)\n","        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)\n","\n","        # 4. 候选记忆单元\n","        # C_tilde = tanh(W_xc * X_t + W_hc * H_{t-1} + b_c)\n","        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)\n","\n","        # 5. 更新细胞状态\n","        # C_t = F_t * C_{t-1} + I_t * C_tilde\n","        C = F * C + I * C_tilda\n","\n","        # 6. 更新隐状态\n","        # H_t = O_t * tanh(C_t)\n","        H = O * torch.tanh(C)\n","\n","        # 7. 计算输出\n","        # Y_t = W_hq * H_t + b_q\n","        Y = (H @ W_hq) + b_q\n","\n","        outputs.append(Y)\n","\n","    # 拼接所有时间步的输出\n","    return torch.cat(outputs, dim=0), (H, C)\n"],"metadata":{"id":"cgJwz1XR6qyt","executionInfo":{"status":"ok","timestamp":1769495096884,"user_tz":-480,"elapsed":1,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## 依旧定义一个可自定义前传函数的RNN类！"],"metadata":{"id":"Gi7jkwEa6va1"}},{"cell_type":"code","source":["class RNNModel():\n","    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n","\n","    def __init__(self, vocab_size, num_hiddens, device,\n","                 get_params, init_state, forward_fn):\n","        \"\"\"\n","        初始化 RNN 模型。\n","\n","        参数：\n","            vocab_size (int): 词汇表大小，即输入和输出的特征数量。\n","            num_hiddens (int): 隐藏单元数量，决定 RNN 的记忆容量。\n","            device (torch.device): 计算设备。\n","            get_params (function): 获取模型参数的函数。\n","            init_state (function): 初始化隐藏状态的函数。\n","            forward_fn (function): RNN 的前向传播函数。\n","        \"\"\"\n","        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n","        # 调用 get_params 初始化权重和偏置\n","        self.params = get_params(vocab_size, num_hiddens, device)\n","        # 记录初始化状态函数和前向传播函数\n","        self.init_state, self.forward_fn = init_state, forward_fn\n","\n","    def __call__(self, X, state):\n","        \"\"\"\n","        执行模型的前向传播。\n","\n","        参数：\n","            X (tensor): 输入数据，形状（批量大小，序列长度）。\n","            state (tuple): 隐藏状态。\n","\n","        返回：\n","            outputs (tensor): 预测结果，形状（时间步数量 * 批量大小，词表大小）。\n","            new_state (tuple): 更新后的隐藏状态。\n","        \"\"\"\n","        # 对输入 X 进行 one-hot 编码并转换为 float32\n","        # 形状：(序列长度，批量大小，词表大小)\n","        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n","        return self.forward_fn(X, state, self.params)\n","\n","    def begin_state(self, batch_size, device):\n","        \"\"\"\n","        初始化隐藏状态。\n","\n","        参数：\n","            batch_size (int): 批量大小。\n","            device (torch.device): 计算设备。\n","\n","        返回：\n","            tuple: 初始化的隐藏状态。\n","        \"\"\"\n","        return self.init_state(batch_size, self.num_hiddens, device)"],"metadata":{"id":"qnu3MuXc6zTT","executionInfo":{"status":"ok","timestamp":1769495096885,"user_tz":-480,"elapsed":1,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["## 依旧是之前的工具函数～"],"metadata":{"id":"ImzVmtpNGerj"}},{"cell_type":"code","source":["import math\n","import torch\n","from torch import nn\n","import matplotlib.pyplot as plt\n","\n","class Accumulator:\n","    \"\"\"\n","    在多个变量上进行累加的工具类\n","    \"\"\"\n","    def __init__(self, n):\n","        \"\"\"\n","        参数:\n","            n (int): 需要累加的变量个数\n","        \"\"\"\n","        self.data = [0.0] * n\n","\n","    def add(self, *args):\n","        \"\"\"\n","        将传入的值逐项累加\n","        \"\"\"\n","        self.data = [a + float(b) for a, b in zip(self.data, args)]\n","\n","    def reset(self):\n","        \"\"\"\n","        清零\n","        \"\"\"\n","        self.data = [0.0] * len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        允许用 metric[i] 的方式访问\n","        \"\"\"\n","        return self.data[idx]\n","\n","def grad_clipping(net, theta):\n","    \"\"\"\n","    梯度裁剪，防止梯度爆炸\n","\n","    参数:\n","        net: 模型（nn.Module 或自定义 RNN）\n","        theta (float): 梯度范数阈值\n","    \"\"\"\n","    if isinstance(net, torch.nn.Module):\n","        params = [p for p in net.parameters() if p.requires_grad]\n","    else:\n","        params = net.params  # RNN 的参数列表\n","\n","    # 计算梯度的 L2 范数\n","    norm = torch.sqrt(\n","        sum(torch.sum(p.grad ** 2) for p in params if p.grad is not None)\n","    )\n","\n","    # 若超过阈值，则按比例缩放\n","    if norm > theta:\n","        for p in params:\n","            if p.grad is not None:\n","                p.grad[:] *= theta / norm\n","\n","def predict(prefix, num_preds, net, vocab, device):\n","    \"\"\"\n","    在给定的前缀字符串之后，使用 RNN 模型生成新的字符序列。\n","\n","    参数：\n","        prefix (str): 生成序列的起始字符串（种子文本）。\n","        num_preds (int): 需要生成的字符数。\n","        net (RNNModelScratch): 训练好的循环神经网络模型。\n","        vocab (Vocab): 词汇表，提供字符与索引的映射关系。\n","        device (torch.device): 计算设备（'cpu' 或 'cuda'）。\n","\n","    返回：\n","        str: 生成的完整文本（包含前缀和预测的新字符）。\n","    \"\"\"\n","    # 初始化 RNN 的隐藏状态，batch_size=1 处理单个序列\n","    state = net.begin_state(batch_size=1, device=device)\n","\n","    # 将 prefix 的第一个字符转换为索引并存入输出列表\n","    outputs = [vocab[prefix[0]]]\n","\n","    # 定义一个 lambda 函数，获取当前最后一个字符的索引并转换为模型输入\n","    get_input = lambda: torch.tensor(\n","        [outputs[-1]], device=device\n","    ).reshape((1, 1))\n","\n","    # 预热期：将 prefix 剩余字符依次输入网络，帮助 RNN 进入适当的状态\n","    for y in prefix[1:]:\n","        _, state = net(get_input(), state)\n","        outputs.append(vocab[y])\n","\n","    # 生成 num_preds 个新的字符\n","    for _ in range(num_preds):\n","        y, state = net(get_input(), state)\n","        outputs.append(int(y.argmax(dim=1).reshape(1)))\n","\n","    # 将输出索引列表转换回字符，并连接成字符串\n","    return ''.join([vocab.idx_to_token[i] for i in outputs])\n","\n","\n","def train_epoch(net, train_iter, loss, optimizer, device, use_random_iter):\n","    state = None\n","    metric = Accumulator(2)  # [total_loss, total_tokens]\n","\n","    for X, Y in train_iter:\n","        # 第一个batch：初始化 state\n","        if state is None or use_random_iter:\n","            state = net.begin_state(batch_size=X.shape[0], device=device)\n","        else:\n","            # 其它batch：detach，避免计算图越来越长导致“卡死”\n","            for s in state:\n","                s.detach_()\n","\n","        y = Y.T.reshape(-1)          # (B, T) -> (T*B,)\n","        X, y = X.to(device), y.to(device)\n","\n","        y_hat, state = net(X, state)\n","        l = loss(y_hat, y.long()).mean()\n","\n","        optimizer.zero_grad()\n","        l.backward()\n","        grad_clipping(net, 1.0)\n","        optimizer.step()\n","\n","        metric.add(l * y.numel(), y.numel())\n","\n","    return math.exp(metric[0] / metric[1])\n","\n","\n","def train(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n","    loss = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.AdamW(net.params, lr=lr)\n","\n","    epochs, ppls = [], []\n","\n","    for epoch in range(num_epochs):\n","        ppl = train_epoch(net, train_iter, loss, optimizer, device, use_random_iter)\n","\n","        print(f\"[epoch {epoch+1:4d}] perplexity = {ppl:.2f}\")\n","\n","        epochs.append(epoch + 1)\n","        ppls.append(ppl)\n","\n","        if (epoch + 1) % 5 == 0:\n","            print(predict('dear gatsby ', 50, net, vocab, device))\n","\n","    plt.figure(figsize=(6, 3))\n","    plt.plot(epochs, ppls)\n","    plt.xlabel('epoch')\n","    plt.ylabel('perplexity')\n","    plt.grid(True)\n","    plt.show()\n","\n","    print(\"\\nFinal sample:\")\n","    print(predict('dear gatsby ', 50, net, vocab, device))\n"],"metadata":{"id":"NiVMg0nzGgu1","executionInfo":{"status":"ok","timestamp":1769495150253,"user_tz":-480,"elapsed":15,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["## 定义模型、开始训练～"],"metadata":{"id":"bD0-dCsVIQcw"}},{"cell_type":"code","source":["vocab_size, num_hiddens, device = len(vocab), 256, \"cpu\"\n","num_epochs, lr = 500, 1\n","\n","model = RNNModel(\n","    len(vocab),\n","    num_hiddens,\n","    device,\n","    get_lstm_params,\n","    init_lstm_state,\n","    lstm\n",")\n","\n","train(model, train_iter, vocab, lr, num_epochs, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3qgAmksCITHq","executionInfo":{"status":"error","timestamp":1769502364939,"user_tz":-480,"elapsed":7209760,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}},"outputId":"086be737-4e10-46eb-bf74-ee9d44f8c54b"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[epoch    1] perplexity = 2834579031304047171105176549191757210816642942804689158527545995908053475788639461281661954254660796674809903036063702278236818905677639859803901470201485135664499261053671930396851582548770615727029113259864441472948371456.00\n","[epoch    2] perplexity = 198578427630556533446786366889840344815274417502365535137545161677484144583151942632046265894879635474886562205312701306526385757624878497792.00\n","[epoch    3] perplexity = 165124889506886139784434000657907022207374836032581315627398583455796447308359045585399623121941561344.00\n","[epoch    4] perplexity = 11387702519271089242146665370817142800363210893528754111130094184025337520431205053882173532695788300537594845452371142085348676322544108333992716735243841805707615508574235543264684804225016645489422871318025585921611789940948083633171381985958623187042304.00\n","[epoch    5] perplexity = 135997235300096766227345055940349489463445281725313747066418298564778360472148038543471904913373490065752529774854014009461236698335671887670003793806703460352.00\n","dear gatsb<unk> poolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpoolpool\n","[epoch    6] perplexity = 93583002362680326561109578056377545486849793542224429995517719008137564474709753342024203306798932493543158414433628563693589997600267211963037129694405066871358298973230756057472743985152399141914017792.00\n","[epoch    7] perplexity = 2846797766592915593934755070567010781084359484694450626360622466591954761231839230612464196391154625807165429042759406765916291072.00\n","[epoch    8] perplexity = 124995567976383057561644640392054902548482290928031289899160887852546248928267744426338909283978692403941332363334898539602416438190073313283295999257161533581963949944886099808671914171726083129344.00\n","[epoch    9] perplexity = 3213331060130114467925701117164549813781020602408362277615966676639286815774227568041371462465820198834013265165926751546898382848.00\n","[epoch   10] perplexity = 47450232756723723550101823270793243399569886113465804428849530895108032898483941338624319902196644134694916280994291693670442068762105579370918249620032218105959790797265313661498165403040838686535061472093376062592042742792351534574456512388716822528.00\n","dear gatsb<unk> onononononononononononononononononononononononononononononononononononononononononononononononononon\n","[epoch   11] perplexity = 44951090542343161633167580064545730599676691398469252087055285383513001767362494864552165537839474288152655858503815264584145161352553205546570610568305351477174542800946149552135117875231092390453729088575032963966377650259119487992075386880.00\n","[epoch   12] perplexity = 283636050264373499368550863150179141673841160318033307715259817039246933383082665258896171561299609899606680431133897908620646705790976.00\n","[epoch   13] perplexity = 177553889034469731904943073256224130755480064415443095752388757667131022146338646181019648.00\n","[epoch   14] perplexity = 333070669874246873120455567168648441311754875049662598929685532286035874750299177972659145034696324797572341532769721529551081726541824.00\n","[epoch   15] perplexity = 5455240079754354600845978345240541767511691856182553334278146651951937674023471132018498260763577894337507906061745057599460015533431840750623706524783423595398489014117818626469939189082587680809414994907547961817495490170353012121520153034752.00\n","dear gatsb<unk> speakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeakspeak\n","[epoch   16] perplexity = 718414396322967239967372689324153930892682900139117927057381090107361619376076455247670506098636462507863665765265032723258310567697722158234788897488896.00\n","[epoch   17] perplexity = 1318380682783478559844708748737379354979884110258402268337333330569234920008839501109047122635437599409252748717708781027328.00\n","[epoch   18] perplexity = 2438898891763189952293480591053997802532728642490250605737660951029524675258960918466812802738766519788445379511067849244831815961839970883823476937538208815830747937928319449323907940962191857559389526060049629184.00\n","[epoch   19] perplexity = 503151965012349212018773322541227202650833321280979769137453550313282363817630763788853026225129660201860072094455965992955400853933200815200551222883456586260526050808913134928829463613136523206280830768789091846658459046968120433999554074007405812776960.00\n","[epoch   20] perplexity = 513525744454698161014454520820279291044242154344218081493567430599808914599971887918661270963260717765233301168630118480327077333845640427611097771449991261017571292338910730620615111351762101336863593534083188253674962944.00\n","dear gatsb<unk> veryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryvery\n","[epoch   21] perplexity = 6493567216531841032818773281826521221366139753990768315124562485728094406499332988751213456999846375058534131822553941692688686047559680.00\n","[epoch   22] perplexity = 4667371902104844815082111767791443323406885712887366030509856930294245730466607937340904944173056.00\n","[epoch   23] perplexity = 3455477103743145633596640838538265429875205076322091008.00\n","[epoch   24] perplexity = 1565247211744857727863018172077572096.00\n","[epoch   25] perplexity = 1814745761035728128.00\n","dear gatsb<unk> whatthomasdenizenthomaswalterroyaltyroyaltycordialbiloxicrosslyroyaltycrosslycrosslywalterroyaltydeliveredcrosslynotifiesmansionenlargementchickenchickenchickenbuttonsgodknowingcoupcoupveryecstaticmaybroadcorpulentdrippingabysscontemptuouslydaisyjordanpeopleholdholdholdholdholdstandingeasebuildbuildholdernational\n","[epoch   26] perplexity = 362579575473.01\n","[epoch   27] perplexity = 59455123298234539482021888.00\n","[epoch   28] perplexity = 123560145985926871428257122102813139980139888640.00\n","[epoch   29] perplexity = 29222381751517287801239371776.00\n","[epoch   30] perplexity = 1633047584307503992742762177883466561218045310566974716597030063945011043970457975039731957760.00\n","dear gatsb<unk> handsomerememberchallenginglyhandsomeundefinedhandsomeenlargementleechesautomaticallyrequiredrequiredgaudyalongsidehandsomeundefinedundefinedundefinedundefinedgoldvulnerableenjoyoriginalshopsebookscoincidenceherssharperblossomsresultadvantagesadvantagesadvantageseducationchallenginglyalongsidealongsidespiresadvantagessneerspiresaspectaspiredrequiredundefinedadvantagesaspiredundefinedgaudygaudygaudy\n","[epoch   31] perplexity = 457685442300034160451695180354166538821108026260884837670789464474333037121438823552130212585063579648.00\n","[epoch   32] perplexity = 8130403691235803927482506797603366125538240609207234967288467887834177976524964102144.00\n","[epoch   33] perplexity = 2311299264232064850984012228037794263521263042098240193821663855809914741678299665552870561985579618150418540816294085224777981862921774359956332478464.00\n","[epoch   34] perplexity = 2365468465080178398850243592779538951516857259481482202685090959418879106796850340661707805316427992704163659001061237007184723538759716738261057536.00\n","[epoch   35] perplexity = 845257609543237573407575972448111110775348870209686259689093254485867896109355568122793915952289008463157261853727157258506382948053479715766272.00\n","dear gatsb<unk> brightbrightbrightbrightbrightininininininininininininininininininininininininininininininininininininininininininininin\n","[epoch   36] perplexity = 25704574246798335557267324940876093671211729334602433075394913640025878640103282191081603287183185652876394045273931226608135563252652741817244331990288689725964288.00\n","[epoch   37] perplexity = 35307962283283205686815065630813536778346152275383734526058582398919600306449145320446078004930626743016107021508253951718647487681108802511167875131728346456837509567345590272.00\n","[epoch   38] perplexity = 1712067834807638218439546961362222753780956756167658201068140453926597967902126594756847177822065644355228453471664320716700725631860476277304028369312971009946733344980992.00\n","[epoch   39] perplexity = 558864314708784488576691392736493417473485514575362623021635790181209365706595319717551390612039761894227646087168.00\n","[epoch   40] perplexity = 568594933434210462884164582284781909441345360814887679059238328761476470882718586060107398722668765112770239955378110464.00\n","dear gatsb<unk> suggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestionsuggestion\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2172142447.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-3480354844.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_iter, vocab, lr, num_epochs, device, use_random_iter)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_random_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[epoch {epoch+1:4d}] perplexity = {ppl:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3480354844.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(net, train_iter, loss, optimizer, device, use_random_iter)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mgrad_clipping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m                             )\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    245\u001b[0m             )\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    954\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 )\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}