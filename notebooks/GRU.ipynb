{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMCc1yoonAUhXgC98FPBMtt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# GRU"],"metadata":{"id":"bLCAmn6Hms_M"}},{"cell_type":"markdown","source":["## 依旧先构建Vocab！"],"metadata":{"id":"daRfdd4fnHmv"}},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAjsxj_OmfqH","executionInfo":{"status":"ok","timestamp":1769504703888,"user_tz":-480,"elapsed":117,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}},"outputId":"244919eb-bb9f-409e-9528-e1fb8d9c7d3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["===== Vocabulary Preview =====\n","index -> token\n","  0 ->  \n","  1 -> <unk>\n","  2 -> <bos>\n","  3 -> <eos>\n","  4 -> the\n","  5 -> and\n","  6 -> a\n","  7 -> i\n","  8 -> of\n","  9 -> to\n","now -> 74\n","unknown -> 1\n","sentence -> [2, 1686, 29, 3]\n"]}],"source":["import collections\n","import re\n","\n","import math\n","import time\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","file_path = 'novel.txt'\n","\n","def read_txt_file(file_path):\n","    with open(file_path, 'r') as f:\n","        lines = f.readlines()\n","    cleaned_lines = [\n","        re.sub('[^A-Za-z]+', ' ', line).strip().lower()\n","        for line in lines\n","    ]\n","    return cleaned_lines\n","\n","lines = read_txt_file(file_path)\n","\n","def tokenize(lines, token='word'):\n","    if token == 'word':\n","        return [line.split() for line in lines]\n","    elif token == 'char':\n","        return [list(line) for line in lines]\n","    else:\n","        raise ValueError(token)\n","\n","def count_corpus(corpus):\n","    \"\"\"\n","    统计语料中每个 token 出现的次数\n","    tokens:\n","        - 可以是 ['a', 'b', 'c']\n","        - 也可以是 [['a','b'], ['c','d']]\n","    返回：\n","        Counter({'a': 3, 'b': 2, ...})\n","    \"\"\"\n","    all_tokens = []\n","    for line in corpus:          # 一行一行取\n","      for token in line.split():       # 行里一个个单词取\n","          all_tokens.append(token)\n","\n","\n","    return collections.Counter(all_tokens)\n","\n","class Vocab:\n","    def __init__(self, tokens=None):\n","        \"\"\"\n","        构建词表\n","        tokens: token 列表（可以是一维或二维）\n","        \"\"\"\n","        if tokens is None:\n","            tokens = []\n","\n","        # 1. 统计词频\n","        counter = count_corpus(tokens)\n","\n","        # 2. 初始化特殊符号\n","        self.idx_to_token = [' ', '<unk>', '<bos>', '<eos>']\n","        self.token_to_idx = {\n","            ' ': 0,\n","            '<unk>': 1,\n","            '<bos>': 2,\n","            '<eos>': 3,\n","        }\n","\n","        # 3. 按频率从高到低加入普通 token\n","        for token, freq in counter.most_common():\n","            if token not in self.token_to_idx:\n","                self.idx_to_token.append(token)\n","                self.token_to_idx[token] = len(self.idx_to_token) - 1\n","\n","    def __len__(self):\n","      return len(self.idx_to_token)\n","\n","    def __getitem__(self, tokens):\n","      # 单个 token\n","      if not isinstance(tokens, (list, tuple)):\n","          return self.token_to_idx.get(tokens, self.token_to_idx['<unk>'])\n","\n","      # token 列表\n","      indices = []\n","      for token in tokens:\n","          indices.append(self[token])\n","      return indices\n","\n","\n","    def print_vocab(self, n=10):\n","      print(\"===== Vocabulary Preview =====\")\n","      print(\"index -> token\")\n","      for i in range(min(n, len(self.idx_to_token))):\n","          print(f\"{i:>3} -> {self.idx_to_token[i]}\")\n","\n","vocab = Vocab(lines)\n","\n","vocab.print_vocab(n=10)\n","\n","print(\"now ->\", vocab['now'])\n","print(\"unknown ->\", vocab['xyz'])\n","print(\"sentence ->\", vocab[['<bos>', 'dear', 'gatsby', '<eos>']])"]},{"cell_type":"markdown","source":["## 以及依旧构建dataloader！！"],"metadata":{"id":"kcJRWp6snT_L"}},{"cell_type":"code","source":["import torch\n","def build_corpus_ids(lines, vocab):\n","    # 把text转为数字\n","    words = []\n","    for line in lines:\n","        if line.strip():\n","            words += line.split()\n","    return torch.tensor([vocab[w] for w in words], dtype=torch.long)\n","\n","\n","def train_iter_sequential_simple(corpus_ids, batch_size, num_steps, device='cpu'):\n","    corpus_ids = corpus_ids.to(device)\n","    N = corpus_ids.numel()\n","    assert N > batch_size * (num_steps + 1), \"语料太短，batch_size*num_steps 太大了\"\n","\n","    # 截断到能整除 batch_size\n","    n = (N - 1) // batch_size * batch_size\n","    Xs = corpus_ids[:n].reshape(batch_size, -1)\n","    Ys = corpus_ids[1:n+1].reshape(batch_size, -1)\n","\n","    batches = []   # ★ 所有 batch 放在这里\n","\n","    L = Xs.shape[1]\n","    for t in range(0, L - num_steps + 1, num_steps):\n","        X = Xs[:, t:t+num_steps]\n","        Y = Ys[:, t:t+num_steps]\n","        batches.append((X, Y))   # ★ 收集起来\n","\n","    return batches\n","\n","\n","\n","\n","corpus_ids = build_corpus_ids(lines, vocab)\n","print(\"Total tokens:\", corpus_ids.shape)\n","\n","batch_size = 32\n","num_steps = 35\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Using device:\", device)\n","\n","train_iter = train_iter_sequential_simple(\n","    corpus_ids, batch_size, num_steps, device\n",")\n","\n","\n","for X, Y in train_iter:\n","    print(\"X shape:\", X.shape)   # (batch_size, num_steps)\n","    print(\"Y shape:\", Y.shape)\n","    print(\"X[0]:\", X[0])\n","    print(\"Y[0]:\", Y[0])\n","    break\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"icbuAXBAnUnU","executionInfo":{"status":"ok","timestamp":1769504703964,"user_tz":-480,"elapsed":70,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}},"outputId":"95f38465-b918-40c3-a23a-2fe0bfe0334b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Total tokens: torch.Size([53029])\n","Using device: cpu\n","X shape: torch.Size([32, 35])\n","Y shape: torch.Size([32, 35])\n","X[0]: tensor([   4,   81,   76,  487,    8,    4,  223,   29,   38,  487,   58,   25,\n","           4,  299,    8,  419, 1028,   10,    4,  420,  326,    5,  194,   92,\n","        1513,    8,    4,  201,   19,   63, 1219,    5,   18,  273,   63])\n","Y[0]: tensor([  81,   76,  487,    8,    4,  223,   29,   38,  487,   58,   25,    4,\n","         299,    8,  419, 1028,   10,    4,  420,  326,    5,  194,   92, 1513,\n","           8,    4,  201,   19,   63, 1219,    5,   18,  273,   63, 1965])\n"]}]},{"cell_type":"markdown","source":["## 初始化GRU参数！"],"metadata":{"id":"VtBc8w73nZr7"}},{"cell_type":"code","source":["import torch\n","\n","def get_gru_params(vocab_size, num_hiddens, device):\n","    # 输入维度等于输出维度，即词汇表大小\n","    num_inputs = num_outputs = vocab_size\n","\n","    def normal(shape):\n","        return torch.randn(size=shape, device=device) * 0.01\n","\n","    def two_W_one_b():\n","        \"\"\"\n","        每一组线性变换包含三个参数:\n","        1. W_x: 输入到隐藏状态的权重矩阵\n","        2. W_h: 隐藏状态到隐藏状态的权重矩阵\n","        3. b: 偏置项（初始化为 0）\n","\n","        维度:\n","        - W_x: (num_inputs, num_hiddens)\n","        - W_h: (num_hiddens, num_hiddens)\n","        - b:   (num_hiddens,)\n","        \"\"\"\n","        return (\n","            normal((num_inputs, num_hiddens)),     # W_x\n","            normal((num_hiddens, num_hiddens)),    # W_h\n","            torch.zeros(num_hiddens, device=device)  # b\n","        )\n","\n","    # ===== 初始化 GRU 的 3 组门控机制参数 =====\n","    W_xz, W_hz, b_z = two_W_one_b()  # 更新门 (Update Gate)\n","    W_xr, W_hr, b_r = two_W_one_b()  # 重置门 (Reset Gate)\n","    W_xh, W_hh, b_h = two_W_one_b()  # 候选隐藏状态 (Candidate Hidden State)\n","\n","    # ===== 初始化 GRU 的输出层参数 =====\n","    W_hq = normal((num_hiddens, num_outputs))   # 隐藏层 -> 输出层\n","    b_q = torch.zeros(num_outputs, device=device)\n","\n","    # 将所有参数放入列表，便于优化和训练\n","    params = [\n","        W_xz, W_hz, b_z,   # 更新门参数\n","        W_xr, W_hr, b_r,   # 重置门参数\n","        W_xh, W_hh, b_h,   # 候选隐藏状态参数\n","        W_hq, b_q          # 输出层参数\n","    ]\n","\n","    # 开启梯度\n","    for param in params:\n","        param.requires_grad_(True)\n","\n","    return params\n"],"metadata":{"id":"3_1bzoKanbV4","executionInfo":{"status":"ok","timestamp":1769504703966,"user_tz":-480,"elapsed":1,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## 定义GRU前传函数！"],"metadata":{"id":"PDgKhuEqqarU"}},{"cell_type":"code","source":["import torch\n","\n","# GRU 只有一个隐状态\n","def init_gru_state(batch_size, num_hiddens, device):\n","    return (torch.zeros((batch_size, num_hiddens), device=device), )\n","\n","def gru(inputs, state, params):\n","    \"\"\"\n","    GRU（门控循环单元）的前向传播（不依赖 PyTorch 的 nn.GRU）\n","\n","    参数：\n","    - inputs: 输入序列，形状 (seq_len, batch_size, input_size)，每个时间步输入 X_t\n","    - state: (H,)，隐藏状态元组，H 形状 (batch_size, hidden_size)\n","    - params: GRU 所需的所有训练参数\n","\n","    返回：\n","    - outputs: 所有时间步的输出，形状 (seq_len * batch_size, output_size)\n","    - state: 更新后的隐藏状态 (H,)\n","    \"\"\"\n","\n","    # 解包参数\n","    W_xz, W_hz, b_z = params[0:3]   # 更新门 (Update Gate)\n","    W_xr, W_hr, b_r = params[3:6]   # 重置门 (Reset Gate)\n","    W_xh, W_hh, b_h = params[6:9]   # 候选隐藏状态\n","    W_hq, b_q       = params[9:]    # 输出层参数\n","\n","    H, = state                      # 当前隐藏状态\n","    outputs = []\n","\n","    # 遍历输入序列（按时间步）\n","    for X in inputs:\n","        \"\"\"\n","        GRU 计算流程：\n","        1. 计算更新门 Z_t\n","        2. 计算重置门 R_t\n","        3. 计算候选隐藏状态 H_tilda\n","        4. 计算新的隐藏状态 H_t\n","        5. 计算最终输出 Y_t\n","        \"\"\"\n","\n","        # 1. 更新门 Z_t\n","        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)\n","\n","        # 2. 重置门 R_t\n","        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)\n","\n","        # 3. 候选隐藏状态 H_tilda\n","        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)\n","\n","        # 4. 新的隐藏状态 H_t\n","        H = Z * H + (1 - Z) * H_tilda\n","\n","        # 5. 输出 Y_t\n","        Y = H @ W_hq + b_q\n","\n","        # 保存当前时间步输出\n","        outputs.append(Y)\n","\n","    # 拼接所有时间步的输出\n","    return torch.cat(outputs, dim=0), (H,)\n"],"metadata":{"id":"ZbrZ3FyIqc2Z","executionInfo":{"status":"ok","timestamp":1769504704007,"user_tz":-480,"elapsed":34,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## 依旧定义一个可自定义前传函数的RNN类！"],"metadata":{"id":"iWovuXQlrb-k"}},{"cell_type":"code","source":["class RNNModel():\n","    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n","\n","    def __init__(self, vocab_size, num_hiddens, device,\n","                 get_params, init_state, forward_fn):\n","        \"\"\"\n","        初始化 RNN 模型。\n","\n","        参数：\n","            vocab_size (int): 词汇表大小，即输入和输出的特征数量。\n","            num_hiddens (int): 隐藏单元数量，决定 RNN 的记忆容量。\n","            device (torch.device): 计算设备。\n","            get_params (function): 获取模型参数的函数。\n","            init_state (function): 初始化隐藏状态的函数。\n","            forward_fn (function): RNN 的前向传播函数。\n","        \"\"\"\n","        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n","        # 调用 get_params 初始化权重和偏置\n","        self.params = get_params(vocab_size, num_hiddens, device)\n","        # 记录初始化状态函数和前向传播函数\n","        self.init_state, self.forward_fn = init_state, forward_fn\n","\n","    def __call__(self, X, state):\n","        \"\"\"\n","        执行模型的前向传播。\n","\n","        参数：\n","            X (tensor): 输入数据，形状（批量大小，序列长度）。\n","            state (tuple): 隐藏状态。\n","\n","        返回：\n","            outputs (tensor): 预测结果，形状（时间步数量 * 批量大小，词表大小）。\n","            new_state (tuple): 更新后的隐藏状态。\n","        \"\"\"\n","        # 对输入 X 进行 one-hot 编码并转换为 float32\n","        # 形状：(序列长度，批量大小，词表大小)\n","        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n","        return self.forward_fn(X, state, self.params)\n","\n","    def begin_state(self, batch_size, device):\n","        \"\"\"\n","        初始化隐藏状态。\n","\n","        参数：\n","            batch_size (int): 批量大小。\n","            device (torch.device): 计算设备。\n","\n","        返回：\n","            tuple: 初始化的隐藏状态。\n","        \"\"\"\n","        return self.init_state(batch_size, self.num_hiddens, device)"],"metadata":{"id":"Bcg4aifCrcpo","executionInfo":{"status":"ok","timestamp":1769504704010,"user_tz":-480,"elapsed":1,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["## 依旧是之前的工具函数～"],"metadata":{"id":"ZbnRK6Ssrhha"}},{"cell_type":"code","source":["import math\n","import torch\n","from torch import nn\n","import matplotlib.pyplot as plt\n","\n","class Accumulator:\n","    \"\"\"\n","    在多个变量上进行累加的工具类\n","    \"\"\"\n","    def __init__(self, n):\n","        \"\"\"\n","        参数:\n","            n (int): 需要累加的变量个数\n","        \"\"\"\n","        self.data = [0.0] * n\n","\n","    def add(self, *args):\n","        \"\"\"\n","        将传入的值逐项累加\n","        \"\"\"\n","        self.data = [a + float(b) for a, b in zip(self.data, args)]\n","\n","    def reset(self):\n","        \"\"\"\n","        清零\n","        \"\"\"\n","        self.data = [0.0] * len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        允许用 metric[i] 的方式访问\n","        \"\"\"\n","        return self.data[idx]\n","\n","def grad_clipping(net, theta):\n","    \"\"\"\n","    梯度裁剪，防止梯度爆炸\n","\n","    参数:\n","        net: 模型（nn.Module 或自定义 RNN）\n","        theta (float): 梯度范数阈值\n","    \"\"\"\n","    if isinstance(net, torch.nn.Module):\n","        params = [p for p in net.parameters() if p.requires_grad]\n","    else:\n","        params = net.params  # RNN 的参数列表\n","\n","    # 计算梯度的 L2 范数\n","    norm = torch.sqrt(\n","        sum(torch.sum(p.grad ** 2) for p in params if p.grad is not None)\n","    )\n","\n","    # 若超过阈值，则按比例缩放\n","    if norm > theta:\n","        for p in params:\n","            if p.grad is not None:\n","                p.grad[:] *= theta / norm\n","\n","def predict(prefix, num_preds, net, vocab, device):\n","    \"\"\"\n","    在给定的前缀字符串之后，使用 RNN 模型生成新的字符序列。\n","\n","    参数：\n","        prefix (str): 生成序列的起始字符串（种子文本）。\n","        num_preds (int): 需要生成的字符数。\n","        net (RNNModelScratch): 训练好的循环神经网络模型。\n","        vocab (Vocab): 词汇表，提供字符与索引的映射关系。\n","        device (torch.device): 计算设备（'cpu' 或 'cuda'）。\n","\n","    返回：\n","        str: 生成的完整文本（包含前缀和预测的新字符）。\n","    \"\"\"\n","    # 初始化 RNN 的隐藏状态，batch_size=1 处理单个序列\n","    state = net.begin_state(batch_size=1, device=device)\n","\n","    # 将 prefix 的第一个字符转换为索引并存入输出列表\n","    outputs = [vocab[prefix[0]]]\n","\n","    # 定义一个 lambda 函数，获取当前最后一个字符的索引并转换为模型输入\n","    get_input = lambda: torch.tensor(\n","        [outputs[-1]], device=device\n","    ).reshape((1, 1))\n","\n","    # 预热期：将 prefix 剩余字符依次输入网络，帮助 RNN 进入适当的状态\n","    for y in prefix[1:]:\n","        _, state = net(get_input(), state)\n","        outputs.append(vocab[y])\n","\n","    # 生成 num_preds 个新的字符\n","    for _ in range(num_preds):\n","        y, state = net(get_input(), state)\n","        outputs.append(int(y.argmax(dim=1).reshape(1)))\n","\n","    # 将输出索引列表转换回字符，并连接成字符串\n","    return ''.join([vocab.idx_to_token[i] for i in outputs])\n","\n","\n","def train_epoch(net, train_iter, loss, optimizer, device, use_random_iter):\n","    state = None\n","    metric = Accumulator(2)  # [total_loss, total_tokens]\n","\n","    for X, Y in train_iter:\n","        # 第一个batch：初始化 state\n","        if state is None or use_random_iter:\n","            state = net.begin_state(batch_size=X.shape[0], device=device)\n","        else:\n","            # 其它batch：detach，避免计算图越来越长导致“卡死”\n","            for s in state:\n","                s.detach_()\n","\n","        y = Y.T.reshape(-1)          # (B, T) -> (T*B,)\n","        X, y = X.to(device), y.to(device)\n","\n","        y_hat, state = net(X, state)\n","        l = loss(y_hat, y.long()).mean()\n","\n","        optimizer.zero_grad()\n","        l.backward()\n","        grad_clipping(net, 1.0)\n","        optimizer.step()\n","\n","        metric.add(l * y.numel(), y.numel())\n","\n","    return math.exp(metric[0] / metric[1])\n","\n","\n","def train(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n","    loss = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.AdamW(net.params, lr=lr)\n","\n","    epochs, ppls = [], []\n","\n","    for epoch in range(num_epochs):\n","        ppl = train_epoch(net, train_iter, loss, optimizer, device, use_random_iter)\n","\n","        print(f\"[epoch {epoch+1:4d}] perplexity = {ppl:.2f}\")\n","\n","        epochs.append(epoch + 1)\n","        ppls.append(ppl)\n","\n","        if (epoch + 1) % 5 == 0:\n","            print(predict('dear gatsby ', 50, net, vocab, device))\n","\n","    plt.figure(figsize=(6, 3))\n","    plt.plot(epochs, ppls)\n","    plt.xlabel('epoch')\n","    plt.ylabel('perplexity')\n","    plt.grid(True)\n","    plt.show()\n","\n","    print(\"\\nFinal sample:\")\n","    print(predict('dear gatsby ', 50, net, vocab, device))\n"],"metadata":{"id":"tCRSRHtErh-F","executionInfo":{"status":"ok","timestamp":1769504704035,"user_tz":-480,"elapsed":25,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["## 定义模型、开始训练～"],"metadata":{"id":"zInfQ1AbrluE"}},{"cell_type":"code","source":["vocab_size, num_hiddens, device = len(vocab), 256, \"cpu\"\n","num_epochs, lr = 500, 0.001\n","\n","model = RNNModel(\n","    len(vocab),\n","    num_hiddens,\n","    device,\n","    get_gru_params,\n","    init_gru_state,\n","    gru\n",")\n","\n","train(model, train_iter, vocab, lr, num_epochs, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"aPCS2WRkrnbP","executionInfo":{"status":"error","timestamp":1769509955088,"user_tz":-480,"elapsed":4417300,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}},"outputId":"7bc60e30-cee2-455e-bb36-ed50c79552ce"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[epoch    1] perplexity = 1526.51\n","[epoch    2] perplexity = 780.48\n","[epoch    3] perplexity = 762.38\n","[epoch    4] perplexity = 758.22\n","[epoch    5] perplexity = 757.16\n","dear gatsb<unk> thethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethe\n","[epoch    6] perplexity = 757.73\n","[epoch    7] perplexity = 754.42\n","[epoch    8] perplexity = 752.33\n","[epoch    9] perplexity = 750.05\n","[epoch   10] perplexity = 710.80\n","dear gatsb<unk> thethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethethe\n","[epoch   11] perplexity = 643.56\n","[epoch   12] perplexity = 576.75\n","[epoch   13] perplexity = 515.37\n","[epoch   14] perplexity = 449.19\n","[epoch   15] perplexity = 399.02\n","dear gatsb<unk> thehouseofthehouseoftheroomoftheroomoftheroomoftheroomoftheroomoftheroomoftheroomoftheroomoftheroomoftheroomoftheroomoftheroomoftheroomoftheroomoftheroom\n","[epoch   16] perplexity = 340.46\n","[epoch   17] perplexity = 391.18\n","[epoch   18] perplexity = 262.35\n","[epoch   19] perplexity = 232.08\n","[epoch   20] perplexity = 209.54\n","dear gatsb<unk> thehouseandtheprojectgutenbergelectronicworksandthenandtheroomandtheroomandtheroomandtheroomandtheroomandtheroomandtheroomandtheroomandtheroomandtheroomandtheroomandtheroomandtheroomand\n","[epoch   21] perplexity = 184.31\n","[epoch   22] perplexity = 161.64\n","[epoch   23] perplexity = 143.48\n","[epoch   24] perplexity = 129.06\n","[epoch   25] perplexity = 119.48\n","dear gatsb<unk> theroomandthenihadbeengoingtobealittleandihadbeenbeenintheprojectgutenbergelectronicworksintheprojectgutenbergelectronicworkstheprojectgutenbergelectronicworksintheprojectgutenbergelectronicworkstheprojectgutenbergelectronicworksandtheprojectgutenbergelectronic\n","[epoch   26] perplexity = 108.78\n","[epoch   27] perplexity = 97.43\n","[epoch   28] perplexity = 87.10\n","[epoch   29] perplexity = 78.21\n","[epoch   30] perplexity = 70.66\n","dear gatsb<unk> thehouseandiwasalittleandiwasgoingtoseethephoneandtheroomwasalittlelateriwasgoingtoseeyourealittleandiwasalittleandiwasalittleandiwasalittleandiwasa\n","[epoch   31] perplexity = 64.60\n","[epoch   32] perplexity = 59.22\n","[epoch   33] perplexity = 53.38\n","[epoch   34] perplexity = 47.48\n","[epoch   35] perplexity = 42.68\n","dear gatsb<unk> gatsbyseyesandithoughtitwasalittleandiwasgoingtobealittlelaterimgoingtogotogotoseeyoucantyouaboutthephoneandiwasgoingtobealittlelateridontknowamoment\n","[epoch   36] perplexity = 39.05\n","[epoch   37] perplexity = 35.84\n","[epoch   38] perplexity = 32.71\n","[epoch   39] perplexity = 29.54\n","[epoch   40] perplexity = 26.88\n","dear gatsb<unk> gatsbyseyesandiwenttotheroomandthenithoughtitwasamanwhohadbeensoicantknowiwasgoingtogotobedandthenithoughtitwasalittlelaterbutitwasalittlelaterbutisaw\n","[epoch   41] perplexity = 24.59\n","[epoch   42] perplexity = 22.67\n","[epoch   43] perplexity = 20.69\n","[epoch   44] perplexity = 18.44\n","[epoch   45] perplexity = 16.62\n","dear gatsb<unk> tombuchananintheroomandasiftheyweresomuchasiftofactthatitwasamanwhohadalittlelaterbutitsallrightnowandtombuchananandhereyescameintotheroomandasiftheyweregoingtosee\n","[epoch   46] perplexity = 15.37\n","[epoch   47] perplexity = 14.32\n","[epoch   48] perplexity = 13.32\n","[epoch   49] perplexity = 12.29\n","[epoch   50] perplexity = 11.19\n","dear gatsb<unk> shehadbeeninvitedinitwasasmalltownhehadalittleandgivenwithamiserablewayeverybodydididhaveanicegirlheretomsheturnedtomeandiwenttowesteggvillagetotheotherroomthebutlersfacehis\n","[epoch   51] perplexity = 10.24\n","[epoch   52] perplexity = 9.49\n","[epoch   53] perplexity = 8.78\n","[epoch   54] perplexity = 8.19\n","[epoch   55] perplexity = 7.62\n","dear gatsb<unk> shewenttoheadidontwanttodoyouregoingtogetawayidontwanttodoyouregoingtogetawayidontwanttodoyouregoingtogetawayidontwanttodoyouregoingto\n","[epoch   56] perplexity = 7.04\n","[epoch   57] perplexity = 6.55\n","[epoch   58] perplexity = 6.15\n","[epoch   59] perplexity = 5.75\n","[epoch   60] perplexity = 5.49\n","dear gatsb<unk> oldsportdontknowwhatyoullcallupthechurchandyoualotofclippingsaboutitivegottogetmixedupinitshousethatnighthehadbeguntowhistletherosarytunelesslyinsidepleasehaveyouabetterlookathim\n","[epoch   61] perplexity = 5.28\n","[epoch   62] perplexity = 4.92\n","[epoch   63] perplexity = 4.56\n","[epoch   64] perplexity = 4.21\n","[epoch   65] perplexity = 3.93\n","dear gatsb<unk> butwenttohimwithvacanteyesthroughasortofappealasthoughshedidntlikeitshedoesntyousheansweredshortlyathingtobeaboutitbutiwasgoingtoseeherunderthehouseiwasonandthenhehadchanged\n","[epoch   66] perplexity = 3.71\n","[epoch   67] perplexity = 3.54\n","[epoch   68] perplexity = 3.40\n","[epoch   69] perplexity = 3.24\n","[epoch   70] perplexity = 3.08\n","dear gatsb<unk> saidcatherinewhodidisaidbutshemarriedmeandtomlookedbackatmeandthenbackatthecoronerwithdeterminedeyesnottobeaboutfromamonthagoatarowthatskeepinginwhichhehadafightwithamanwhohad\n","[epoch   71] perplexity = 2.94\n","[epoch   72] perplexity = 2.80\n","[epoch   73] perplexity = 2.67\n","[epoch   74] perplexity = 2.56\n","[epoch   75] perplexity = 2.47\n","dear gatsb<unk> shetookherintoajewellerystoretogatsbymysisterandspendtheautomobileswhichturnedouthercuriousandhurriedintwoofhisfadedeyesbegantoberenderedihadnochoiceexcepttocuthimoffthereivegotmyhandsuntiliwent\n","[epoch   76] perplexity = 2.37\n","[epoch   77] perplexity = 2.30\n","[epoch   78] perplexity = 2.22\n","[epoch   79] perplexity = 2.15\n","[epoch   80] perplexity = 2.08\n","dear gatsb<unk> shewenttofindoutontheothergirlandithoughtitwasoneofthecrisesofmylifeweregoingtotownhenoddedidontbelieveithesaidsuddenlythaticouldntgettomydisappointmentthathehadneverwastrying\n","[epoch   81] perplexity = 2.01\n","[epoch   82] perplexity = 1.94\n","[epoch   83] perplexity = 1.89\n","[epoch   84] perplexity = 1.84\n","[epoch   85] perplexity = 1.80\n","dear gatsb<unk> tomtalkedforgatsbyshouseatnineoclockgatsbyhadgoneintolovingdaisyhislifehadbeenhoveringintheshouseididntknowhimidliketogetoneofthecitywhereyouarelocatedintheunitedstatesyouwillhave\n","[epoch   86] perplexity = 1.74\n","[epoch   87] perplexity = 1.70\n","[epoch   88] perplexity = 1.67\n","[epoch   89] perplexity = 1.65\n","[epoch   90] perplexity = 1.61\n","dear gatsb<unk> shewenttofindthethreeoclockopeningthedoorandfoundouthiswalletwithtremblingfingerslookatthedoorsheaddedasifneithercouldanybodyinanotherworldandnoonemansvoicewascallingithoughtitwasabiggolftournamentthere\n","[epoch   91] perplexity = 1.57\n","[epoch   92] perplexity = 1.53\n","[epoch   93] perplexity = 1.49\n","[epoch   94] perplexity = 1.46\n","[epoch   95] perplexity = 1.44\n","dear gatsb<unk> tomtalkedforherhandasheturnedaroundfromgatsbyandthenbackintomyneighboursstoppedandwaswidetheretobetheirhusbandseveneyesstoohemusthavestoodforafewminuteslaterthatmrsloanehadalittleselfcontrolselfcontrol\n","[epoch   96] perplexity = 1.41\n","[epoch   97] perplexity = 1.39\n","[epoch   98] perplexity = 1.37\n","[epoch   99] perplexity = 1.36\n","[epoch  100] perplexity = 1.34\n","dear gatsb<unk> tomcameoutofthehouseisatdownforahundredyardsbeyondanditsdriverhurriedbacktowheremyrtlehadadignifiedandindifferentwaybrokedownentirelyandresortedtoflankattacksatintervalssheappearedsuddenlyathissidelikeanangrydiamondandhissed\n","[epoch  101] perplexity = 1.32\n","[epoch  102] perplexity = 1.31\n","[epoch  103] perplexity = 1.30\n","[epoch  104] perplexity = 1.28\n","[epoch  105] perplexity = 1.27\n","dear gatsb<unk> butshewantedtobealonehetookoffhisdancewithsuchaproprietaryhasteandlookedaroundsopossessivelyatthefurniturethatiwonderedifshelivedherebutwhenilovedyoushelovesmehewasnottogethimfromthecitybutfrom\n","[epoch  106] perplexity = 1.25\n","[epoch  107] perplexity = 1.23\n","[epoch  108] perplexity = 1.22\n","[epoch  109] perplexity = 1.20\n","[epoch  110] perplexity = 1.19\n","dear gatsb<unk> thatwouldneverhaveastoryabouthertootillshecriedatagirlfriendineverlovedyougetamassageandawaveandacollarforthedogandoneofthosecutelittleashtrayswhereyoutouchamonthagoyouthinkhedidnt\n","[epoch  111] perplexity = 1.17\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1278966749.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-3480354844.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_iter, vocab, lr, num_epochs, device, use_random_iter)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_random_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[epoch {epoch+1:4d}] perplexity = {ppl:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3480354844.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(net, train_iter, loss, optimizer, device, use_random_iter)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mgrad_clipping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}