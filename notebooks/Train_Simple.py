import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import os

# --- å¯¼å…¥æˆ‘ä»¬ä¹‹å‰å†™çš„æ¨¡å— (è‡ªå®¶å†›ç«åº“) ---
# å¿…é¡»ç¡®ä¿æ–‡ä»¶åå®Œå…¨ä¸€è‡´ï¼Œå¦åˆ™æŠ¥é”™
from U_Net_Skeleton import SimpleUNet        # å¯¹åº” 07_UNet_Skeleton.py (è¯·é‡å‘½åæˆ–ä¿®æ”¹import)
from Diffusion_Scheduler import LinearNoiseScheduler # å¯¹åº” 08_Diffusion_Scheduler.py
from Dataset_Loader import MedicalImageDataset # å¯¹åº” 09_Dataset_Loader.py

# --- é…ç½®å‚æ•° ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 4 # æ˜¾å­˜å°å°±æ”¹å°
LR = 1e-4
EPOCHS = 100 # è·‘å‡ è½®

def train():
    print(f"ğŸš€ Training on {DEVICE}...")
    
    # 1. å‡†å¤‡æ¨¡å‹ã€è°ƒåº¦å™¨ã€æ•°æ®
    model = SimpleUNet(in_channels=3, context_dim=768).to(DEVICE) # å›¾ç‰‡æ˜¯3é€šé“(RGB)
    scheduler = LinearNoiseScheduler()
    optimizer = optim.AdamW(model.parameters(), lr=LR)
    criterion = nn.MSELoss() # å‡æ–¹è¯¯å·®ï¼šé¢„æµ‹å™ªå£° vs çœŸå®å™ªå£°

    # --- è‡ªåŠ¨å®šä½è·¯å¾„ ---
    # 1. è·å–å½“å‰è„šæœ¬ (Train_Simple.py) æ‰€åœ¨çš„ç»å¯¹è·¯å¾„
    current_script_dir = os.path.dirname(os.path.abspath(__file__))

    # 2. ç®—å‡ºé¡¹ç›®æ ¹ç›®å½• (notebooks çš„ä¸Šä¸€çº§)
    project_root = os.path.dirname(current_script_dir)

    # 3. æ‹¼å‡º data æ–‡ä»¶å¤¹çš„ç»å¯¹è·¯å¾„
    data_path = os.path.join(project_root, 'data')

    print(f"ğŸ“ Script Location: {current_script_dir}")
    print(f"ğŸ“‚ Looking for Data in: {data_path}")

    # --- åŠ è½½æ•°æ® ---
    dataset = MedicalImageDataset(img_dir=data_path, image_size=32)
    if len(dataset) == 0:
        print("âŒ æ²¡æ•°æ®è·‘ä¸äº†ï¼è¯·åœ¨ data æ–‡ä»¶å¤¹é‡Œæ”¾å‡ å¼ å›¾ã€‚")
        return
        
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    # 2. è®­ç»ƒå¾ªç¯
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        
        for batch_idx, images in enumerate(dataloader):
            images = images.to(DEVICE)
            curr_batch = images.shape[0]
            
            # A. éšæœºé‡‡æ ·æ—¶é—´æ­¥ t (æ¯”å¦‚ç¬¬ 50 æ­¥ï¼Œç¬¬ 900 æ­¥...)
            t = torch.randint(0, 1000, (curr_batch,)).to(DEVICE)
            
            # B. ç”Ÿæˆéšæœºå™ªå£° noise (è¿™å°±æ˜¯æˆ‘ä»¬è¦é¢„æµ‹çš„ç›®æ ‡)
            noise = torch.randn_like(images).to(DEVICE)
            
            # C. åŠ å™ª (Forward Process): x_t = scheduler(x_0, t, noise)
            # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨äº†æˆ‘ä»¬æ˜¨å¤©å†™çš„æ•°å­¦å…¬å¼ï¼
            noisy_images = scheduler.add_noise(images, noise, t)
            
            # D. å‡è£…æœ‰ä¸€ä¸ªæ–‡æœ¬æç¤º (å®é™…æ¯•è®¾è¦ç”¨ CLIP ç¼–ç ï¼Œè¿™é‡Œå…ˆç”¨éšæœºå‘é‡ä»£æ›¿)
            # å½¢çŠ¶: (Batch, 77, 768)
            dummy_text_context = torch.randn(curr_batch, 77, 768).to(DEVICE)
            
            # E. æ¨¡å‹é¢„æµ‹å™ªå£° (Reverse Process)
            predicted_noise = model(noisy_images, dummy_text_context)
            
            # F. ç®— Loss & åå‘ä¼ æ’­
            loss = criterion(predicted_noise, noise) # çŒœçš„å™ªå£° vs çœŸçš„å™ªå£°
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
        # æ‰“å°è¿›åº¦
        avg_loss = total_loss / len(dataloader)
        if (epoch+1) % 10 == 0:
            print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.5f}")
            
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²å­¦ä¼šå¦‚ä½•å»å™ªã€‚")
    # torch.save(model.state_dict(), "medi_diff_v1.pth")

if __name__ == "__main__":
    train()
