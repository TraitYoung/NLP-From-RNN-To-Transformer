{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c6152",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    这是 GPT 的心脏：带掩码的自注意力机制 (Masked Self-Attention)。\n",
    "    面试必问点：\n",
    "    1. 为什么要除以 sqrt(head_dim)? -> 防止 Softmax 梯度消失。\n",
    "    2. 为什么要加 Mask? -> 保证模型只能看见过去，不能看见未来 (自回归属性)。\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_head, max_len=1024):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0 # 确保能被整除\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_model // n_head\n",
    "        \n",
    "        # 定义 Q, K, V 的映射矩阵\n",
    "        # 相比于分别定义三个 Linear，合并成一个再 split 效率更高\n",
    "        self.c_attn = nn.Linear(d_model, 3 * d_model)\n",
    "        \n",
    "        # 输出投影层\n",
    "        self.c_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 定义因果掩码 (Causal Mask)\n",
    "        # 这是一个下三角矩阵，用来盖住右上角（未来的信息）\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(max_len, max_len))\n",
    "                                     .view(1, 1, max_len, max_len))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # Batch_size, Time_step (Sequence Length), Channels (Embed Dim)\n",
    "        \n",
    "        # 1. 计算 Q, K, V\n",
    "        # qkv shape: (B, T, 3 * C) -> split -> (B, T, C)\n",
    "        q, k, v = self.c_attn(x).split(C, dim=2)\n",
    "        \n",
    "        # 2. 变换形状以适应多头注意力 (Multi-Head)\n",
    "        # (B, T, n_head, head_dim) -> transpose -> (B, n_head, T, head_dim)\n",
    "        # 这样 transpose 后，n_head 维度在外，可以并行计算所有头\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 3. 计算注意力分数 (Scaled Dot-Product Attention)\n",
    "        # att shape: (B, n_head, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        \n",
    "        # 4. 应用因果掩码 (Masking)\n",
    "        # 将 mask 为 0 的位置填入 -inf，这样 Softmax 后就会变成 0\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        \n",
    "        # 5. 归一化概率\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        \n",
    "        # 6. 聚合 Value\n",
    "        y = att @ v # (B, n_head, T, head_dim)\n",
    "        \n",
    "        # 7. 拼回原始形状\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        return self.c_proj(y)\n",
    "\n",
    "# --- 测试代码 (Run this to verify) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 模拟一个 Batch=1, 长度=10个词, 维度=64 的输入\n",
    "    d_model = 64\n",
    "    n_head = 4\n",
    "    x = torch.randn(1, 10, d_model)\n",
    "    \n",
    "    # 初始化模块\n",
    "    block = CausalSelfAttention(d_model=d_model, n_head=n_head)\n",
    "    \n",
    "    # 前向传播\n",
    "    output = block(x)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"Transformer Block forward pass successful!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
