{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c6152",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    这是 GPT 的心脏：带掩码的自注意力机制 (Masked Self-Attention)。\n",
    "    面试必问点：\n",
    "    1. 为什么要除以 sqrt(head_dim)? -> 防止 Softmax 梯度消失。\n",
    "    2. 为什么要加 Mask? -> 保证模型只能看见过去，不能看见未来 (自回归属性)。\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_head, max_len=1024):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0 # 确保能被整除\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_model // n_head\n",
    "        \n",
    "        # 定义 Q, K, V 的映射矩阵\n",
    "        # 相比于分别定义三个 Linear，合并成一个再 split 效率更高\n",
    "        self.c_attn = nn.Linear(d_model, 3 * d_model)\n",
    "        \n",
    "        # 输出投影层\n",
    "        self.c_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 定义因果掩码 (Causal Mask)\n",
    "        # 这是一个下三角矩阵，用来盖住右上角（未来的信息）\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(max_len, max_len))\n",
    "                                     .view(1, 1, max_len, max_len))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # Batch_size, Time_step (Sequence Length), Channels (Embed Dim)\n",
    "        \n",
    "        # 1. 计算 Q, K, V\n",
    "        # qkv shape: (B, T, 3 * C) -> split -> (B, T, C)\n",
    "        q, k, v = self.c_attn(x).split(C, dim=2)\n",
    "        \n",
    "        # 2. 变换形状以适应多头注意力 (Multi-Head)\n",
    "        # (B, T, n_head, head_dim) -> transpose -> (B, n_head, T, head_dim)\n",
    "        # 这样 transpose 后，n_head 维度在外，可以并行计算所有头\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 3. 计算注意力分数 (Scaled Dot-Product Attention)\n",
    "        # att shape: (B, n_head, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        \n",
    "        # 4. 应用因果掩码 (Masking)\n",
    "        # 将 mask 为 0 的位置填入 -inf，这样 Softmax 后就会变成 0\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        \n",
    "        # 5. 归一化概率\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        \n",
    "        # 6. 聚合 Value\n",
    "        y = att @ v # (B, n_head, T, head_dim)\n",
    "        \n",
    "        # 7. 拼回原始形状\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        return self.c_proj(y)\n",
    "\n",
    "# --- 测试代码 (Run this to verify) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 模拟一个 Batch=1, 长度=10个词, 维度=64 的输入\n",
    "    d_model = 64\n",
    "    n_head = 4\n",
    "    x = torch.randn(1, 10, d_model)\n",
    "    \n",
    "    # 初始化模块\n",
    "    block = CausalSelfAttention(d_model=d_model, n_head=n_head)\n",
    "    \n",
    "    # 前向传播\n",
    "    output = block(x)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"Transformer Block forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab0f1c8",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 2. The Bridge: Cross-Attention (For Diffusion / medi-diff)\n",
    "# ==========================================\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    这是 Stable Diffusion / U-Net 的关键组件。\n",
    "    作用：让图像生成过程“听懂”文本提示词 (Prompt)。\n",
    "    \n",
    "    面试必问点：\n",
    "    Q: Q, K, V 分别来自哪里？\n",
    "    A: Q 来自图像特征 (Latent Image)，K 和 V 来自文本编码 (CLIP Text Embedding)。\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_context, n_head):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_model // n_head\n",
    "        \n",
    "        # Q 来自图像 (U-Net 的中间层特征)\n",
    "        self.to_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # K, V 来自文本 (CLIP 的输出 context) !!!\n",
    "        # 注意：d_context 通常是 CLIP 的维度 (例如 768)，可能与 d_model 不同\n",
    "        self.to_k = nn.Linear(d_context, d_model, bias=False)\n",
    "        self.to_v = nn.Linear(d_context, d_model, bias=False)\n",
    "        \n",
    "        self.to_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        # x: 图像特征 (Batch, Pixels, Channel) -> 比如 (1, 1024, 320)\n",
    "        # context: 文本特征 (Batch, Token_Len, Channel) -> 比如 (1, 77, 768)\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        h = self.n_head\n",
    "        \n",
    "        # 1. 计算 Q (图像), K (文本), V (文本)\n",
    "        q = self.to_q(x).view(B, -1, h, self.head_dim).transpose(1, 2)\n",
    "        k = self.to_k(context).view(B, -1, h, self.head_dim).transpose(1, 2)\n",
    "        v = self.to_v(context).view(B, -1, h, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 2. 计算注意力分数 (注意：Cross Attention 通常不需要 Causal Mask)\n",
    "        # 图像的任何一个像素都可以看文本的任何一个词\n",
    "        dots = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        \n",
    "        # 3. 聚合信息\n",
    "        out = attn @ v\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "# --- 联合测试代码 ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Testing Self-Attention (GPT mode)...\")\n",
    "    # ... (之前的测试代码不用动) ...\n",
    "    \n",
    "    print(\"-\" * 20)\n",
    "    print(\"Testing Cross-Attention (Diffusion mode)...\")\n",
    "    # 模拟 U-Net 中的情况\n",
    "    unet_dim = 320   # U-Net 特征维度\n",
    "    text_dim = 768   # CLIP 文本维度\n",
    "    \n",
    "    # 模拟图像输入: (Batch=1, Pixels=32*32=1024, Dim=320)\n",
    "    dummy_img = torch.randn(1, 1024, unet_dim)\n",
    "    # 模拟文本输入: (Batch=1, Tokens=77, Dim=768)\n",
    "    dummy_text = torch.randn(1, 77, text_dim)\n",
    "    \n",
    "    cross_block = CrossAttention(d_model=unet_dim, d_context=text_dim, n_head=8)\n",
    "    output = cross_block(dummy_img, dummy_text)\n",
    "    \n",
    "    print(f\"Image Input: {dummy_img.shape}\")\n",
    "    print(f\"Text Input:  {dummy_text.shape}\")\n",
    "    print(f\"Fused Output:{output.shape}\") \n",
    "    print(\"Cross-Attention success! Image is now guided by Text.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
