{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5YxhIW2XNgH8XCkv2I+0w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 翻译数据集"],"metadata":{"id":"kvVndz39xeY2"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215},"id":"hSxYFHoAxRJS","executionInfo":{"status":"error","timestamp":1769522289255,"user_tz":-480,"elapsed":19,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}},"outputId":"38928673-9be6-4798-e938-ea9375d886bb"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'cmn.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2538896743.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m with open(os.path.join('cmn.txt'), 'r',\n\u001b[0m\u001b[1;32m      3\u001b[0m           encoding='utf-8') as f:\n\u001b[1;32m      4\u001b[0m     \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cmn.txt'"]}],"source":["import os\n","with open(os.path.join('cmn.txt'), 'r',\n","          encoding='utf-8') as f:\n","    raw_text = f.read()\n","\n","print(raw_text[:200])"]},{"cell_type":"code","source":["def preprocess(text):\n","    \"\"\"预处理“英语-法语”数据集\"\"\"\n","\n","    def no_space(char, prev_char):\n","        \"\"\"\n","        判断是否在当前字符前添加空格。\n","        如果当前字符是 `, . ! ?` 之一，并且前一个字符不是空格，则返回 True（需要添加空格）。\n","        \"\"\"\n","        return char in set(',.!?') and prev_char != ' '\n","\n","    # '\\u202f' 是窄不间断空格 (narrow no-break space)\n","    # '\\xa0' 是不间断空格 (no-break space)\n","    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n","\n","    # 将文本转换为小写，标准化文本格式\n","    text = text.lower()\n","\n","    # 在单词和标点符号之间插入空格\n","    out = [\n","        ' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n","        for i, char in enumerate(text)  # 遍历文本的每个字符\n","    ]\n","\n","    # 将字符列表合并为字符串\n","    return ''.join(out)\n","\n","# 示例输入\n","text = preprocess(raw_text)  # 预处理原始文本\n","print(text[:80])  # 打印前 80 个字符"],"metadata":{"id":"7vDwgDOby5RX","executionInfo":{"status":"aborted","timestamp":1769522289251,"user_tz":-480,"elapsed":219,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize(text, num_examples=None):\n","    \"\"\"对“英语—法语”翻译数据集进行【单词级】词元化 (Tokenization)\"\"\"\n","\n","    # 初始化存储源语言（英语）和目标语言（法语）句子的列表\n","    source, target = [], []\n","\n","    # 遍历文本的每一行，并按 `\\n` (换行符) 拆分文本\n","    for i, line in enumerate(text.split('\\n')):\n","\n","        # 如果指定了 `num_examples`，则只处理最多 `num_examples` 行\n","        if num_examples and i > num_examples:\n","            break\n","\n","        # 按 `\\t` (制表符) 拆分行，得到英语句子和对应的法语翻译\n","        parts = line.split('\\t')\n","\n","        # 只有当该行包含两个部分（即英语和法语）时才进行处理\n","        if len(parts) == 2:\n","\n","            # 将英语句子按空格拆分为单词列表，并添加到 `source` (源语言) 列表\n","            source.append(parts[0].split(' '))\n","\n","            # 将法语句子按空格拆分为单词列表，并添加到 `target` (目标语言) 列表\n","            target.append(parts[1].split(' '))\n","\n","    # 返回处理后的两个列表，分别存储英语和法语的 token 列表\n","    return source, target\n","\n","# 词元化整个数据集\n","# 注意：这里的 text 应该是上面读取到的 raw_text\n","source, target = tokenize(raw_text, num_examples=30)\n","\n","# 打印处理后的结果示例\n","for s, t in zip(source, target):\n","    print('english:', s, '中文:', t)\n","    # 如果只想看前几个示例，可以加一个 break 或限制循环次数"],"metadata":{"id":"UgLRP8NazTk8","executionInfo":{"status":"aborted","timestamp":1769522289252,"user_tz":-480,"elapsed":218,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import collections\n","import re\n","\n","import math\n","import time\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import collections\n","\n","def count_corpus(tokens):\n","    \"\"\"统计词元频率\"\"\"\n","    # 如果 tokens 是二维列表（如 source），将其展平为一维\n","    if len(tokens) == 0 or isinstance(tokens[0], list):\n","        tokens = [token for line in tokens for token in line]\n","    return collections.Counter(tokens)\n","\n","\n","class Vocab:\n","    def __init__(self, tokens=None):\n","        if tokens is None:\n","            tokens = []\n","\n","        # 1. 统计词频\n","        counter = count_corpus(tokens)\n","\n","        # 2. 初始化特殊符号 (这里的 ' ' 通常作为 <pad> 使用)\n","        self.idx_to_token = ['<pad>', '<unk>', '<bos>', '<eos>']\n","        self.token_to_idx = {\n","            '<pad>': 0,\n","            '<unk>': 1,\n","            '<bos>': 2,\n","            '<eos>': 3,\n","        }\n","\n","        # 3. 按频率从高到低加入普通 token\n","        # counter.most_common() 返回 (token, freq) 的列表\n","        for token, freq in counter.most_common():\n","            if token not in self.token_to_idx:\n","                self.idx_to_token.append(token)\n","                self.token_to_idx[token] = len(self.idx_to_token) - 1\n","\n","    def __len__(self):\n","        return len(self.idx_to_token)\n","\n","    def __getitem__(self, tokens):\n","        # 如果是单个单词，返回其索引；如果没找到，返回 <unk> 的索引\n","        if isinstance(tokens, str):\n","            return self.token_to_idx.get(tokens, self.token_to_idx['<unk>'])\n","        # 如果是单词列表，递归处理\n","        return [self[token] for token in tokens]\n","\n","    def to_tokens(self, indices):\n","        \"\"\"根据索引转回单词 (方便调试)\"\"\"\n","        if not isinstance(indices, (list, tuple)):\n","            return self.idx_to_token[indices]\n","        return [self.idx_to_token[index] for index in indices]\n","\n","    def print_vocab(self, n=10):\n","        print(\"===== Vocabulary Preview =====\")\n","        print(\"index -> token\")\n","        for i in range(min(n, len(self.idx_to_token))):\n","            print(f\"{i:>3} -> {self.idx_to_token[i]}\")\n","\n","\n","\n","# 假设 source 已经通过前面的 tokenize(raw_text) 生成了\n","# 示例数据: source = [['Go.'], ['Hi.'], ['Run!'], ['Run!']]\n","\n","# 初始化英语词表\n","src_vocab = Vocab(source)\n","\n","# 打印词表预览\n","src_vocab.print_vocab(15)\n","\n","# 测试：将一个句子转为索引\n","example_sentence = source[0] # 比如 ['Go.']\n","indices = src_vocab[example_sentence]\n","\n","print(f\"\\nExample sentence: {example_sentence}\")\n","print(f\"Indices: {indices}\")\n","print(f\"Back to tokens: {src_vocab.to_tokens(indices)}\")"],"metadata":{"id":"qYrUHYwIzgzQ","executionInfo":{"status":"aborted","timestamp":1769522289298,"user_tz":-480,"elapsed":263,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## truancate 和 padding！"],"metadata":{"id":"fUguAwFM0j0I"}},{"cell_type":"code","source":["def truncate_pad(line, num_steps, padding_token):\n","    \"\"\"截断或填充文本序列\"\"\"\n","    if len(line) > num_steps:\n","        return line[:num_steps]  # 截断\n","    return line + [padding_token] * (num_steps - len(line))  # 填充\n","\n","# 示例逻辑\n","# truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>'])\n","\n","def build_array(lines, vocab, num_steps):\n","    \"\"\"将机器翻译的文本序列转换成小批量数据 (batch)\"\"\"\n","    # 1. 将每个单词转换成对应的索引\n","    lines = [vocab[l] for l in lines]\n","\n","    # 2. 在每个句子结尾添加 <eos> 标记\n","    lines = [l + [vocab['<eos>']] for l in lines]\n","\n","    # 3. 截断或填充，确保长度固定为 num_steps，并转为 Tensor\n","    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n","\n","    # 4. 计算有效长度 (排除 <pad> 后的实际单词数)\n","    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n","\n","    return array, valid_len\n","\n","from torch.utils import data\n","\n","def load_array(data_arrays, batch_size, is_train=True):\n","    \"\"\"\n","    构造一个 PyTorch 数据迭代器。\n","\n","    参数:\n","    data_arrays (tuple): 包含 (src_array, src_valid_len, tgt_array, tgt_valid_len) 的元组\n","    batch_size (int): 每个小批量的大小\n","    is_train (bool): 是否在迭代时打乱数据\n","    \"\"\"\n","    # 1. 将数据封装成 TensorDataset\n","    # *data_arrays 会将元组解包，传入对应的张量\n","    dataset = data.TensorDataset(*data_arrays)\n","\n","    # 2. 返回 DataLoader 实例\n","    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","# 注意：此函数依赖前面定义的 tokenize, Vocab 类, build_array 和 truncate_pad\n","def load_data(batch_size, num_steps, num_examples=600):\n","    \"\"\"返回翻译数据集的迭代器和词表\"\"\"\n","    with open(os.path.join('cmn.txt'), 'r', encoding='utf-8') as f:\n","        raw_text = f.read()\n","\n","    # 假设 preprocess 函数已定义，或者直接使用 raw_text\n","    # text = preprocess(raw_text)\n","    source, target = tokenize(raw_text, num_examples)\n","\n","    # 构建词表\n","    src_vocab = Vocab(source)\n","    tgt_vocab = Vocab(target)\n","\n","    # 生成张量数组\n","    src_array, src_valid_len = build_array(source, src_vocab, num_steps)\n","    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, num_steps)\n","\n","    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n","\n","    # 假设 load_array 是一个从 Tensor 创建 DataLoader 的工具函数\n","    # from utils import load_array\n","    data_iter = load_array(data_arrays, batch_size)\n","\n","    return data_iter, src_vocab, tgt_vocab\n","\n","# 运行示例\n","train_iter, src_vocab, tgt_vocab = load_data(batch_size=2, num_steps=8)\n","\n","\n","# 1. 获取第一个小批量数据\n","# X: 源语言索引, X_valid_len: 源语言有效长度\n","# Y: 目标语言索引, Y_valid_len: 目标语言有效长度\n","X, X_valid_len, Y, Y_valid_len = next(iter(train_iter))\n","\n","print(f\"当前 Batch 大小: {X.shape[0]}, 句子最大长度 (num_steps): {X.shape[1]}\")\n","print(\"-\" * 50)\n","\n","# 2. 遍历并打印这个 Batch 里的每一个 Case\n","for i in range(X.shape[0]):\n","    # 使用 src_vocab 的 idx_to_token 属性进行转换\n","    # X[i] 是一个 tensor，tolist() 转为列表以便索引\n","    src_tokens = [src_vocab.idx_to_token[int(idx)] for idx in X[i]]\n","    tgt_tokens = [tgt_vocab.idx_to_token[int(idx)] for idx in Y[i]]\n","\n","    print(f\"Case {i+1}:\")\n","    print(f\"  [Source]: {' '.join(src_tokens)}\")\n","    print(f\"  [Valid L]: {X_valid_len[i].item()}\")\n","    print(f\"  [Target]: {' '.join(tgt_tokens)}\")\n","    print(f\"  [Valid L]: {Y_valid_len[i].item()}\")\n","    print(\"-\" * 50)"],"metadata":{"id":"EiAwsWJ-0jba","executionInfo":{"status":"aborted","timestamp":1769522289299,"user_tz":-480,"elapsed":262,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Encoder Decoder"],"metadata":{"id":"Ap8aDaYq4s51"}},{"cell_type":"code","source":["from torch import nn\n","\n","class Encoder(nn.Module):\n","    \"\"\"编码器基类\"\"\"\n","    def __init__(self, **kwargs):\n","        super(Encoder, self).__init__(**kwargs)\n","\n","    def forward(self, X, *args):\n","        raise NotImplementedError\n","\n","class Decoder(nn.Module):\n","    \"\"\"解码器基类\"\"\"\n","    def __init__(self, **kwargs):\n","        super(Decoder, self).__init__(**kwargs)\n","\n","    def init_state(self, enc_outputs, *args):\n","        raise NotImplementedError\n","\n","    def forward(self, X, state):\n","        raise NotImplementedError\n","\n","class EncoderDecoder(nn.Module):\n","    \"\"\"编码器-解码器架构的基类\"\"\"\n","    def __init__(self, encoder, decoder, **kwargs):\n","        super(EncoderDecoder, self).__init__(**kwargs)\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, enc_X, dec_X, *args):\n","        enc_outputs = self.encoder(enc_X, *args)\n","        dec_state = self.decoder.init_state(enc_outputs, *args)\n","        return self.decoder(dec_X, dec_state)"],"metadata":{"id":"sT2SfgSh4ubw","executionInfo":{"status":"aborted","timestamp":1769522289299,"user_tz":-480,"elapsed":261,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Seq2SeqEncoder(Encoder):\n","    \"\"\"用于序列到序列学习的循环神经网络编码器\"\"\"\n","    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n","                 dropout=0, **kwargs):\n","        super(Seq2SeqEncoder, self).__init__(**kwargs)\n","        # 嵌入层：将词索引转换为稠密向量\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        # RNN 层：使用多层门控循环单元 (GRU)\n","        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)\n","\n","    def forward(self, X, *args):\n","        # 输入 X 的形状: (batch_size, num_steps) -> [批量大小, 时间步数]\n","\n","        # 1. 特征映射：将索引转为词向量\n","        # 输出形状: (batch_size, num_steps, embed_size)\n","        X = self.embedding(X)\n","\n","        # 2. 维度转换：PyTorch 的 RNN 默认期望 (num_steps, batch_size, embed_size)\n","        # 将“时间步”换到第一维\n","        X = X.permute(1, 0, 2)\n","\n","        # 3. 前向传播\n","        # output 形状: (num_steps, batch_size, num_hiddens) -> 包含所有时刻的隐状态\n","        # state 形状: (num_layers, batch_size, num_hiddens) -> 包含每一层最后时刻的隐状态\n","        output, state = self.rnn(X)\n","\n","        # 返回输出和最终状态，状态将作为解码器的初始状态\n","        return output, state\n","\n","class Seq2SeqDecoder(Decoder):\n","    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n","        super(Seq2SeqDecoder, self).__init__(**kwargs)\n","        # 解码器也有自己的嵌入层\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","\n","        # RNN 层：注意输入维度是 embed_size + num_hiddens\n","        # 因为我们要把当前词向量和编码器传来的“上下文向量”拼在一起作为输入\n","        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)\n","\n","        # 输出层：将隐藏状态映射回词表大小的维度，用于预测下一个词\n","        self.dense = nn.Linear(num_hiddens, vocab_size)\n","\n","    def init_state(self, enc_outputs, *args):\n","        # 接收编码器的输出，并提取其最终隐藏状态作为初始状态\n","        # enc_outputs[1] 即编码器的最终 state\n","        return enc_outputs[1]\n","\n","    def forward(self, X, state):\n","        # 输入 X 形状: (batch_size, num_steps)\n","        # state 形状: (num_layers, batch_size, num_hiddens)\n","\n","        # 1. 词向量化并调整维度 -> (num_steps, batch_size, embed_size)\n","        X = self.embedding(X).permute(1, 0, 2)\n","\n","        # 2. 提取上下文向量：\n","        # 选取编码器最后一层的最后时间步状态作为“上下文”\n","        # state[-1] 形状: (batch_size, num_hiddens)\n","        # 使用 repeat 将其复制，使其在每个时间步都可见 -> (num_steps, batch_size, num_hiddens)\n","        context = state[-1].repeat(X.shape[0], 1, 1)\n","\n","        # 3. 特征拼接：将当前输入 X 与上下文 context 在特征轴 (dim=2) 拼接\n","        # 拼接后形状: (num_steps, batch_size, embed_size + num_hiddens)\n","        X_and_context = torch.cat((X, context), 2)\n","\n","        # 4. 前向传播：传入拼接后的向量和上一时刻的状态\n","        output, state = self.rnn(X_and_context, state)\n","\n","        # 5. 映射输出：通过全连通层，并把维度调回 (batch_size, num_steps, vocab_size)\n","        output = self.dense(output).permute(1, 0, 2)\n","\n","        # 返回预测概率分布和更新后的状态（用于下一个时间步）\n","        return output, state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215},"id":"shBmA0G64x_z","executionInfo":{"status":"error","timestamp":1769522289421,"user_tz":-480,"elapsed":86,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}},"outputId":"2a0c1ea3-c055-41b6-baf1-91acee507c36"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Encoder' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1427185476.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSeq2SeqEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"用于序列到序列学习的循环神经网络编码器\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n\u001b[1;32m      4\u001b[0m                  dropout=0, **kwargs):\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeq2SeqEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Encoder' is not defined"]}]},{"cell_type":"markdown","source":["## mask！"],"metadata":{"id":"Ws2U4uNn6hFk"}},{"cell_type":"code","source":["def sequence_mask(X, valid_len, value=0):\n","    \"\"\"\n","    在序列中屏蔽 (mask) 不相关的项，通常用于对填充部分进行屏蔽。\n","    参数:\n","        X (torch.Tensor): 形状为 (batch_size, maxlen) 的输入张量。\n","        valid_len (torch.Tensor): 形状为 (batch_size,) 的张量，表示每个序列的有效长度。\n","        value (int or float, 可选): 需要屏蔽的位置填充的数值，默认值为 0。\n","    返回:\n","        torch.Tensor: 处理后的张量，其中无效部分被替换为 `value`。\n","    \"\"\"\n","    # 获取输入张量 X 的最大序列长度 (num_steps)，即 X 的第二维度\n","    maxlen = X.size(1)\n","\n","    # 生成一个形状为 (1, maxlen) 的张量，其值为 0 到 maxlen-1\n","    # - `torch.arange(maxlen)` 创建一个 1D 向量 [0, 1, 2, ..., maxlen-1]\n","    # - `device=X.device` 确保计算在相同设备 (如 GPU/CPU) 上进行\n","    # - `[None, :]` 使其扩展为形状 (1, maxlen)，以便与 `valid_len[:, None]` 进行广播\n","    mask = torch.arange(maxlen, dtype=torch.float32, device=X.device)[None, :]\n","\n","    # 生成一个形状为 (batch_size, maxlen) 的布尔掩码矩阵\n","    # - `valid_len[:, None]` 将 `valid_len` 变为形状 (batch_size, 1)，以进行广播\n","    # - `mask < valid_len[:, None]` 逐元素比较：\n","    #   - 若 `mask` 中的索引小于 `valid_len`，则该位置为 True (有效)\n","    #   - 否则，该位置为 False (填充部分需要屏蔽)\n","    mask = mask < valid_len[:, None]\n","\n","    # 利用掩码对 `X` 进行屏蔽 (将无效部分替换为 `value`)\n","    # - `~mask` 取反，将 False (填充部分) 变为 True，True (有效部分) 变为 False\n","    # - `X[~mask] = value` 将 `X` 中无效部分替换为 `value`\n","    X[~mask] = value\n","\n","    return X"],"metadata":{"id":"Leo7C-3n6iRV","executionInfo":{"status":"aborted","timestamp":1769522289425,"user_tz":-480,"elapsed":84,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n","    \"\"\"带遮蔽(masking)的 softmax 交叉熵损失函数\"\"\"\n","    def forward(self, pred, label, valid_len):\n","        \"\"\"\n","        参数:\n","            pred (tensor): 形状 (batch_size, num_steps, vocab_size) -> 预测的概率分布\n","            label (tensor): 形状 (batch_size, num_steps) -> 真实标签\n","            valid_len (tensor): 形状 (batch_size,) -> 每个序列的有效长度 (不包括 <pad>)\n","        \"\"\"\n","        # 创建与 `label` 形状相同的 `weights` 张量, 并全部初始化为 1\n","        weights = torch.ones_like(label)\n","        # 通过 `sequence_mask()` 生成遮蔽 (mask), 过滤填充部分\n","        weights = sequence_mask(weights, valid_len)\n","        # 禁用 CrossEntropyLoss 的默认 `reduction` 选项, 让其计算逐元素损失\n","        self.reduction = 'none'\n","\n","        # 计算未加权的交叉熵损失\n","        # CrossEntropyLoss 期望输入形状为 (batch_size, vocab_size, num_steps), 所以需要 permute\n","        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n","            pred.permute(0, 2, 1), label)\n","\n","        # 计算加权损失: 填充部分的损失变为 0 (不会影响梯度)\n","        # 对 `num_steps` 维度求均值, 得到每个样本的最终损失\n","        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n","\n","        return weighted_loss # **返回每个样本的损失**"],"metadata":{"id":"F8-qdTqj7kU-","executionInfo":{"status":"aborted","timestamp":1769522289426,"user_tz":-480,"elapsed":85,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Accumulator:\n","    \"\"\"\n","    在多个变量上进行累加的工具类\n","    \"\"\"\n","    def __init__(self, n):\n","        \"\"\"\n","        参数:\n","            n (int): 需要累加的变量个数\n","        \"\"\"\n","        self.data = [0.0] * n\n","\n","    def add(self, *args):\n","        \"\"\"\n","        将传入的值逐项累加\n","        \"\"\"\n","        self.data = [a + float(b) for a, b in zip(self.data, args)]\n","\n","    def reset(self):\n","        \"\"\"\n","        清零\n","        \"\"\"\n","        self.data = [0.0] * len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        允许用 metric[i] 的方式访问\n","        \"\"\"\n","        return self.data[idx]\n","\n","def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n","    \"\"\"\n","    训练序列到序列 (Seq2Seq) 模型\n","    \"\"\"\n","    # 1. 权重初始化：使用 Xavier 初始化使模型在训练初期更稳定\n","    def xavier_init_weights(m):\n","        if type(m) == nn.Linear:\n","            nn.init.xavier_uniform_(m.weight)\n","        if type(m) == nn.GRU:\n","            # GRU 有多个内部参数矩阵，需要遍历初始化\n","            for param in m._flat_weights_names:\n","                if \"weight\" in param:\n","                    nn.init.xavier_uniform_(m._parameters[param])\n","\n","    net.apply(xavier_init_weights) # 应用初始化\n","    net.to(device)                 # 模型搬运到 GPU/CPU\n","\n","    optimizer = torch.optim.Adam(net.parameters(), lr=lr) # 使用 Adam 优化器\n","    loss = MaskedSoftmaxCELoss()                          # 使用自定义的带遮蔽损失函数\n","\n","    net.train() # 设置为训练模式\n","\n","    for epoch in range(num_epochs):\n","        # 使用你提供的 Accumulator 类，累加 2 个变量：损失总和、有效词元总数\n","        metric = Accumulator(2)\n","\n","        for batch in data_iter:\n","            optimizer.zero_grad() # 梯度清零\n","\n","            # X: 源语言, X_valid_len: 源语言有效长度\n","            # Y: 目标语言, Y_valid_len: 目标语言有效长度\n","            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n","\n","            # 2. 构造解码器的输入 (强制教学 Teacher Forcing)\n","            # 在 Y (目标句子) 前面拼接一个 <bos> 标记\n","            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0], device=device).reshape(-1, 1)\n","            # dec_input = <bos> + Y 的前 n-1 个词 (丢弃最后一个词以保持步数一致)\n","            # 例如 Y 为 [我, 是, 学生, <eos>]，则输入为 [<bos>, 我, 是, 学生]\n","            dec_input = torch.cat([bos, Y[:, :-1]], 1)\n","\n","            # 3. 前向传播\n","            # Y_hat 形状: (batch_size, num_steps, vocab_size)\n","            Y_hat, _ = net(X, dec_input, X_valid_len)\n","\n","            # 4. 计算损失\n","            # l 返回的是每个样本的平均损失\n","            l = loss(Y_hat, Y, Y_valid_len)\n","            l.sum().backward() # 损失求和后反向传播计算梯度\n","\n","            # 5. 梯度裁剪 (Gradient Clipping)\n","            # 防止 RNN 常见的梯度爆炸问题，将梯度范数限制在 1 以内\n","            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n","\n","            # 6. 更新权重\n","            num_tokens = Y_valid_len.sum() # 计算当前 batch 中所有非 <pad> 的词数\n","            optimizer.step()\n","\n","            # 7. 统计指标\n","            with torch.no_grad():\n","                metric.add(l.sum(), num_tokens)\n","\n","        # 每一轮结束打印一次平均训练损失 (Perplexity 或 Cross Entropy)\n","        # metric[0] 是总损失，metric[1] 是总词数\n","        epoch_loss = metric[0] / metric[1]\n","        print(f'Epoch {epoch + 1}: loss = {epoch_loss:.3f}')\n","\n","    print(\"训练结束！\")"],"metadata":{"id":"rRjKBGEa78Y9","executionInfo":{"status":"aborted","timestamp":1769522289426,"user_tz":-480,"elapsed":85,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n","batch_size, num_steps = 64, 10\n","lr, num_epochs, device = 0.005, 300, \"cpu\"\n","\n","train_iter, src_vocab, tgt_vocab = load_data(batch_size, num_steps)\n","\n","encoder = Seq2SeqEncoder(\n","    len(src_vocab),\n","    embed_size,\n","    num_hiddens,\n","    num_layers,\n","    dropout\n",")\n","\n","decoder = Seq2SeqDecoder(\n","    len(tgt_vocab),\n","    embed_size,\n","    num_hiddens,\n","    num_layers,\n","    dropout\n",")\n","\n","net = EncoderDecoder(encoder, decoder)\n","train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)\n"],"metadata":{"id":"C35KlT0i8zUx","executionInfo":{"status":"aborted","timestamp":1769522289426,"user_tz":-480,"elapsed":384,"user":{"displayName":"Allen Wei","userId":"00558309039811553346"}}},"execution_count":null,"outputs":[]}]}