# NLP: From RNN to Transformer

This repository contains my implementation of NLP fundamentals, moving from basic RNNs to Seq2Seq architectures.

## ðŸ“‚ Project Structure

- **01_Tokenize.ipynb**: Basic text preprocessing and vocabulary building.
- **02_RNN.ipynb**: Implementation of Vanilla RNN from scratch.
- **03_LSTM.ipynb**: Long Short-Term Memory networks to solve vanishing gradients.
- **04_GRU.ipynb**: Gated Recurrent Units for efficient sequence modeling.
- **05_Seq2Seq.ipynb**: Encoder-Decoder architecture for Machine Translation.

## ðŸš€ How to Run

1. Clone the repository:
   ```bash
   git clone [https://github.com/YourUsername/NLP-From-RNN-to-Transformer.git](https://github.com/YourUsername/NLP-From-RNN-to-Transformer.git)